{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWM1BwM8UHQL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from keras.models import Sequential,load_model,Model\n",
        "from keras.layers import Conv2D,MaxPool2D,Dense,Dropout,BatchNormalization,Flatten,Input,AveragePooling2D, GlobalAveragePooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image, ImageOps\n",
        "import seaborn as sns\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras import optimizers\n",
        "import tensorflow as tensorflow\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers.convolutional import SeparableConv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers import SpatialDropout2D\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2necxoxNUw8A",
        "outputId": "03a83590-7bf3-4f64-b300-dcb730eade79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# cloud based platform - storage, mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM04bIRVTYhg"
      },
      "outputs": [],
      "source": [
        "#!rm -rf  /content/drive/MyDrive/Emotion_Detection/utkface_aligned_cropped\n",
        "#!unzip -q  /content/drive/MyDrive/Emotion_Detection/archive.zip -d  /content/drive/MyDrive/Emotion_Detection/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2w067YqPSC"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyIzQk0GUqRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a628f13-32c4-4d42-89c1-7da0713be5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number Of İmages:  23708\n",
            "gender_classes shape:  (23708,)\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/Emotion_Detection/UTKFace/'\n",
        "#/content/drive/MyDrive/Emotion_Detection/crop_part1\n",
        "os.chdir(path)\n",
        "file_list = os.listdir() #Go path and list files\n",
        "print(\"Number Of İmages: \", len(file_list))\n",
        "#Gender\n",
        "gender = []\n",
        "for i in file_list:\n",
        "    gender.append(i.split('_')[1])\n",
        "\n",
        "gender_labels= [\"Male\",\"Female\"]\n",
        "\n",
        "gender_classes = []\n",
        "y_gender= []\n",
        "for i in gender:\n",
        "    i= int(i)\n",
        "    if i== 0:\n",
        "        gender_classes.append(0)\n",
        "    else:\n",
        "        gender_classes.append(1)\n",
        "    y_gender.append(i)\n",
        "\n",
        "gender_classes= np.array(gender_classes)\n",
        "print(\"gender_classes shape: \",gender_classes.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tMFNN73JfFi"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "age = []\n",
        "genders = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhlZmEwUCPhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0544e4ff-ef5c-4fbe-a27e-72e9b5958bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras_preprocessing.image.affine_transformations import apply_brightness_shift\n",
        "from keras_preprocessing.image.affine_transformations import apply_brightness_shift\n",
        "\n",
        "#for i in os.listdir(path)[0:9000]:\n",
        "for i in os.listdir(path):\n",
        "    split = i.split('_')\n",
        "    if (int(split[0])) <=20:\n",
        "        age.append(0)\n",
        "    elif (int(split[0])) > 20 and (int(split[0])) <=40:\n",
        "        age.append(1)\n",
        "    elif (int(split[0])) > 40 and (int(split[0])) <=60:\n",
        "        age.append(2)\n",
        "    elif (int(split[0])) > 60 and (int(split[0])) <=80:\n",
        "        age.append(3)\n",
        "    elif (int(split[0])) > 80: \n",
        "        age.append(4)\n",
        "        print(age)\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikg-ozGiKjZj"
      },
      "outputs": [],
      "source": [
        "age = np.array(age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqkNeBZYQz_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e66c158-b52e-4ccb-8b68-0977a41d143b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23708,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "age.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLfQfY7QKtVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6716e7-009e-4ccc-827a-b3625802d21f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "np.unique(age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23dUdO57eH9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d0afe8-78d8-48e9-fa48-e77b9f874255"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "age[11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo_Ky34066Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e67811-f823-4648-9aa2-7f09529f34f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23708,)\n"
          ]
        }
      ],
      "source": [
        "## age_continuous\n",
        "age_cont = []\n",
        "for i in file_list:\n",
        "    age_cont.append(int(i.split(\"_\")[0]))\n",
        "    \n",
        "age_cont = np.array(age_cont)\n",
        "print(age_cont.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53jdPGuzI0FR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a7658fa2-2692-45b8-9fba-9695829f5e20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Emotion_Detection/UTKFace/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "os.chdir(path)\n",
        "file_list = os.listdir()\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxqa1LgVLpF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33cc800-1c87-4798-bed3-f86ca7d8246d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_data shape:  (23708, 48, 48, 3)\n"
          ]
        }
      ],
      "source": [
        "x_data=[]\n",
        "for file in file_list:\n",
        "    img= cv2.imread(file)\n",
        "    img=cv2.resize(img,dsize = (48,48)) # (200,200)--->(48,48)\n",
        "    x_data.append(img)\n",
        "x_data= np.array(x_data)\n",
        "print(\"x_data shape: \",x_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOoXryVdaEDj"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "input_shape = (48, 48, 3)\n",
        "batch_size=32\n",
        "verbose = 1\n",
        "num_epochs = 100\n",
        "patience = 16\n",
        "base_path = '/content/drive/My Drive/Emotion_Detection/GENDER_AGE/'\n",
        "l2_regularization=0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKAndtY6q_xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb011059-ac95-4548-c7ca-0e2a7e8c2eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples in Training: (18966, 48, 48, 3)\n",
            "Samples in Testing: (4742, 48, 48, 3)\n",
            "y_train shape:  (18966, 5)\n",
            "y_test shape:  (4742, 5)\n",
            "input shape:  (48, 48, 3)\n"
          ]
        }
      ],
      "source": [
        "## Train-Test Split for age\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_data,age,test_size=0.2,\n",
        "                                               shuffle=True,random_state=42)\n",
        "\n",
        "#one hot encoding\n",
        "y_train= to_categorical(y_train,num_classes=5)\n",
        "y_test= to_categorical(y_test,num_classes=5) \n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255.\n",
        "\n",
        "print(\"Samples in Training:\",x_train.shape)\n",
        "print(\"Samples in Testing:\",x_test.shape)\n",
        "\n",
        "\n",
        "print(\"y_train shape: \",y_train.shape)\n",
        "print(\"y_test shape: \",y_test.shape)\n",
        "\n",
        "input_shape= x_train.shape[1:] #we don't take the number of samples\n",
        "print(\"input shape: \",input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osIHBLZwMVNT"
      },
      "outputs": [],
      "source": [
        "def entry_flow(inputs) :\n",
        "\n",
        "    x = Conv2D(32, 5, strides = 2, padding='same')(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(64,5,padding='same')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    previous_block_activation = x\n",
        "\n",
        "    for size in [32, 64, 256] :\n",
        "\n",
        "        x = SeparableConv2D(size, 5, padding='same')(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = SeparableConv2D(size, 5, padding='same')(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "  \n",
        "        x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n",
        "\n",
        "        x = tensorflow.keras.layers.Add()([x, residual])\n",
        "        previous_block_activation = x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWn3Vhh1EHbI"
      },
      "outputs": [],
      "source": [
        "def middle_flow(x, num_blocks=14) :\n",
        "\n",
        "    previous_block_activation = x\n",
        "\n",
        "    for _ in range(num_blocks) :\n",
        "\n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 5, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = tensorflow.keras.layers.Add()([x, previous_block_activation])\n",
        "        previous_block_activation = x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fwipntEELLx"
      },
      "outputs": [],
      "source": [
        "def exit_flow(x) :\n",
        "\n",
        "    previous_block_activation = x\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(312, 5, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(624, 5, padding='same')(x) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    residual = Conv2D(624, 1, strides=2, padding='same')(previous_block_activation)\n",
        "    x = tensorflow.keras.layers.Add()([x, residual])\n",
        "\n",
        "\n",
        "    x = SeparableConv2D(312, 5, padding='same')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = SeparableConv2D(312, 3, padding='same')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(5, activation='softmax')(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p__RzQwaEO_O"
      },
      "outputs": [],
      "source": [
        "img_input = Input(shape=(48, 48, 3))\n",
        "output = exit_flow(middle_flow(entry_flow(img_input)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIhbOg_lETNT"
      },
      "outputs": [],
      "source": [
        "age_model = Model(img_input, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsv3Uobf9iKM"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)     \n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "train1 = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "test1 = test_datagen.flow(x_test, y_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY62QKrl8c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcb476e-36b7-460d-a4e9-7876eb5f2df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "592/593 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.5482\n",
            "Epoch 1: val_loss improved from inf to 0.11384, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.01.hdf5\n",
            "593/593 [==============================] - 63s 73ms/step - loss: 0.1198 - accuracy: 0.5484 - val_loss: 0.1138 - val_accuracy: 0.5784 - lr: 0.0010\n",
            "Epoch 2/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.6078\n",
            "Epoch 2: val_loss did not improve from 0.11384\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 0.1059 - accuracy: 0.6079 - val_loss: 0.1348 - val_accuracy: 0.5148 - lr: 0.0010\n",
            "Epoch 3/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.6305\n",
            "Epoch 3: val_loss improved from 0.11384 to 0.09843, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.03.hdf5\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 0.1004 - accuracy: 0.6306 - val_loss: 0.0984 - val_accuracy: 0.6447 - lr: 0.0010\n",
            "Epoch 4/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.6488\n",
            "Epoch 4: val_loss improved from 0.09843 to 0.09565, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.04.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0956 - accuracy: 0.6488 - val_loss: 0.0956 - val_accuracy: 0.6398 - lr: 0.0010\n",
            "Epoch 5/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.6695\n",
            "Epoch 5: val_loss did not improve from 0.09565\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 0.0911 - accuracy: 0.6698 - val_loss: 0.0975 - val_accuracy: 0.6385 - lr: 0.0010\n",
            "Epoch 6/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.6746\n",
            "Epoch 6: val_loss improved from 0.09565 to 0.09242, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.06.hdf5\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0889 - accuracy: 0.6748 - val_loss: 0.0924 - val_accuracy: 0.6653 - lr: 0.0010\n",
            "Epoch 7/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0863 - accuracy: 0.6870\n",
            "Epoch 7: val_loss improved from 0.09242 to 0.08465, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.07.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0863 - accuracy: 0.6872 - val_loss: 0.0847 - val_accuracy: 0.6898 - lr: 0.0010\n",
            "Epoch 8/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.6910\n",
            "Epoch 8: val_loss improved from 0.08465 to 0.08423, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.08.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0844 - accuracy: 0.6912 - val_loss: 0.0842 - val_accuracy: 0.6932 - lr: 0.0010\n",
            "Epoch 9/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.6987\n",
            "Epoch 9: val_loss did not improve from 0.08423\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 0.0825 - accuracy: 0.6987 - val_loss: 0.0847 - val_accuracy: 0.6862 - lr: 0.0010\n",
            "Epoch 10/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.7055\n",
            "Epoch 10: val_loss improved from 0.08423 to 0.07854, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.10.hdf5\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 0.0812 - accuracy: 0.7055 - val_loss: 0.0785 - val_accuracy: 0.7100 - lr: 0.0010\n",
            "Epoch 11/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.7094\n",
            "Epoch 11: val_loss did not improve from 0.07854\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 0.0801 - accuracy: 0.7094 - val_loss: 0.0791 - val_accuracy: 0.7178 - lr: 0.0010\n",
            "Epoch 12/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.7133\n",
            "Epoch 12: val_loss did not improve from 0.07854\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 0.0793 - accuracy: 0.7133 - val_loss: 0.0803 - val_accuracy: 0.7035 - lr: 0.0010\n",
            "Epoch 13/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.7191\n",
            "Epoch 13: val_loss did not improve from 0.07854\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 0.0775 - accuracy: 0.7191 - val_loss: 0.0791 - val_accuracy: 0.7029 - lr: 0.0010\n",
            "Epoch 14/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.7242\n",
            "Epoch 14: val_loss did not improve from 0.07854\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "593/593 [==============================] - 39s 65ms/step - loss: 0.0769 - accuracy: 0.7242 - val_loss: 0.0803 - val_accuracy: 0.7092 - lr: 0.0010\n",
            "Epoch 15/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.7440\n",
            "Epoch 15: val_loss improved from 0.07854 to 0.07296, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.15.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0722 - accuracy: 0.7440 - val_loss: 0.0730 - val_accuracy: 0.7341 - lr: 1.0000e-04\n",
            "Epoch 16/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.7442\n",
            "Epoch 16: val_loss improved from 0.07296 to 0.07215, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.16.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0716 - accuracy: 0.7442 - val_loss: 0.0721 - val_accuracy: 0.7412 - lr: 1.0000e-04\n",
            "Epoch 17/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.7503\n",
            "Epoch 17: val_loss improved from 0.07215 to 0.07138, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.17.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0704 - accuracy: 0.7503 - val_loss: 0.0714 - val_accuracy: 0.7391 - lr: 1.0000e-04\n",
            "Epoch 18/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.7517\n",
            "Epoch 18: val_loss improved from 0.07138 to 0.07033, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.18.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0702 - accuracy: 0.7517 - val_loss: 0.0703 - val_accuracy: 0.7461 - lr: 1.0000e-04\n",
            "Epoch 19/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.7531\n",
            "Epoch 19: val_loss improved from 0.07033 to 0.06970, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.19.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.0695 - accuracy: 0.7531 - val_loss: 0.0697 - val_accuracy: 0.7461 - lr: 1.0000e-04\n",
            "Epoch 20/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.7553\n",
            "Epoch 20: val_loss improved from 0.06970 to 0.06956, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.20.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0690 - accuracy: 0.7552 - val_loss: 0.0696 - val_accuracy: 0.7472 - lr: 1.0000e-04\n",
            "Epoch 21/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.7565\n",
            "Epoch 21: val_loss did not improve from 0.06956\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 0.0687 - accuracy: 0.7565 - val_loss: 0.0697 - val_accuracy: 0.7493 - lr: 1.0000e-04\n",
            "Epoch 22/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.7582\n",
            "Epoch 22: val_loss improved from 0.06956 to 0.06923, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.22.hdf5\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 0.0683 - accuracy: 0.7582 - val_loss: 0.0692 - val_accuracy: 0.7503 - lr: 1.0000e-04\n",
            "Epoch 23/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.7555\n",
            "Epoch 23: val_loss improved from 0.06923 to 0.06896, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.23.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.0685 - accuracy: 0.7555 - val_loss: 0.0690 - val_accuracy: 0.7526 - lr: 1.0000e-04\n",
            "Epoch 24/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.7576\n",
            "Epoch 24: val_loss did not improve from 0.06896\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0680 - accuracy: 0.7576 - val_loss: 0.0690 - val_accuracy: 0.7524 - lr: 1.0000e-04\n",
            "Epoch 25/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.7633\n",
            "Epoch 25: val_loss improved from 0.06896 to 0.06866, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.25.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.0671 - accuracy: 0.7632 - val_loss: 0.0687 - val_accuracy: 0.7560 - lr: 1.0000e-04\n",
            "Epoch 26/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.7610\n",
            "Epoch 26: val_loss did not improve from 0.06866\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0676 - accuracy: 0.7610 - val_loss: 0.0692 - val_accuracy: 0.7522 - lr: 1.0000e-04\n",
            "Epoch 27/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.7629\n",
            "Epoch 27: val_loss did not improve from 0.06866\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0670 - accuracy: 0.7629 - val_loss: 0.0691 - val_accuracy: 0.7503 - lr: 1.0000e-04\n",
            "Epoch 28/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.7634\n",
            "Epoch 28: val_loss did not improve from 0.06866\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0670 - accuracy: 0.7634 - val_loss: 0.0688 - val_accuracy: 0.7493 - lr: 1.0000e-04\n",
            "Epoch 29/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.7682\n",
            "Epoch 29: val_loss did not improve from 0.06866\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0661 - accuracy: 0.7682 - val_loss: 0.0696 - val_accuracy: 0.7484 - lr: 1.0000e-04\n",
            "Epoch 30/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.7686\n",
            "Epoch 30: val_loss improved from 0.06866 to 0.06821, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.30.hdf5\n",
            "593/593 [==============================] - 43s 72ms/step - loss: 0.0662 - accuracy: 0.7686 - val_loss: 0.0682 - val_accuracy: 0.7552 - lr: 1.0000e-05\n",
            "Epoch 31/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.7648\n",
            "Epoch 31: val_loss improved from 0.06821 to 0.06811, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.31.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.0661 - accuracy: 0.7650 - val_loss: 0.0681 - val_accuracy: 0.7556 - lr: 1.0000e-05\n",
            "Epoch 32/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.7666\n",
            "Epoch 32: val_loss did not improve from 0.06811\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0658 - accuracy: 0.7666 - val_loss: 0.0681 - val_accuracy: 0.7560 - lr: 1.0000e-05\n",
            "Epoch 33/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.7673\n",
            "Epoch 33: val_loss improved from 0.06811 to 0.06804, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.33.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.0655 - accuracy: 0.7673 - val_loss: 0.0680 - val_accuracy: 0.7560 - lr: 1.0000e-05\n",
            "Epoch 34/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.7674\n",
            "Epoch 34: val_loss did not improve from 0.06804\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0656 - accuracy: 0.7674 - val_loss: 0.0681 - val_accuracy: 0.7556 - lr: 1.0000e-05\n",
            "Epoch 35/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.7661\n",
            "Epoch 35: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0657 - accuracy: 0.7661 - val_loss: 0.0681 - val_accuracy: 0.7537 - lr: 1.0000e-05\n",
            "Epoch 36/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.7686\n",
            "Epoch 36: val_loss did not improve from 0.06804\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0654 - accuracy: 0.7686 - val_loss: 0.0681 - val_accuracy: 0.7533 - lr: 1.0000e-06\n",
            "Epoch 37/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.7693\n",
            "Epoch 37: val_loss improved from 0.06804 to 0.06803, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.37.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.0656 - accuracy: 0.7693 - val_loss: 0.0680 - val_accuracy: 0.7535 - lr: 1.0000e-06\n",
            "Epoch 38/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.7713\n",
            "Epoch 38: val_loss did not improve from 0.06803\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0651 - accuracy: 0.7710 - val_loss: 0.0681 - val_accuracy: 0.7535 - lr: 1.0000e-06\n",
            "Epoch 39/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.7696\n",
            "Epoch 39: val_loss did not improve from 0.06803\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0654 - accuracy: 0.7696 - val_loss: 0.0681 - val_accuracy: 0.7539 - lr: 1.0000e-06\n",
            "Epoch 40/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.7673\n",
            "Epoch 40: val_loss did not improve from 0.06803\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0660 - accuracy: 0.7671 - val_loss: 0.0681 - val_accuracy: 0.7537 - lr: 1.0000e-07\n",
            "Epoch 41/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.7671\n",
            "Epoch 41: val_loss did not improve from 0.06803\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0660 - accuracy: 0.7671 - val_loss: 0.0680 - val_accuracy: 0.7537 - lr: 1.0000e-07\n",
            "Epoch 42/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.7703\n",
            "Epoch 42: val_loss did not improve from 0.06803\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0653 - accuracy: 0.7703 - val_loss: 0.0681 - val_accuracy: 0.7528 - lr: 1.0000e-07\n",
            "Epoch 43/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.7678\n",
            "Epoch 43: val_loss improved from 0.06803 to 0.06798, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/age_model_Xception.43.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.0655 - accuracy: 0.7678 - val_loss: 0.0680 - val_accuracy: 0.7539 - lr: 1.0000e-07\n",
            "Epoch 44/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.7701\n",
            "Epoch 44: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0656 - accuracy: 0.7701 - val_loss: 0.0680 - val_accuracy: 0.7539 - lr: 1.0000e-07\n",
            "Epoch 45/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.7719\n",
            "Epoch 45: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0650 - accuracy: 0.7719 - val_loss: 0.0680 - val_accuracy: 0.7537 - lr: 1.0000e-07\n",
            "Epoch 46/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.7702\n",
            "Epoch 46: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0654 - accuracy: 0.7702 - val_loss: 0.0681 - val_accuracy: 0.7541 - lr: 1.0000e-07\n",
            "Epoch 47/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.7691\n",
            "Epoch 47: val_loss did not improve from 0.06798\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0655 - accuracy: 0.7691 - val_loss: 0.0680 - val_accuracy: 0.7541 - lr: 1.0000e-07\n",
            "Epoch 48/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.7672\n",
            "Epoch 48: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0656 - accuracy: 0.7672 - val_loss: 0.0681 - val_accuracy: 0.7543 - lr: 1.0000e-08\n",
            "Epoch 49/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.7685\n",
            "Epoch 49: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0655 - accuracy: 0.7685 - val_loss: 0.0680 - val_accuracy: 0.7537 - lr: 1.0000e-08\n",
            "Epoch 50/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.7728\n",
            "Epoch 50: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0656 - accuracy: 0.7728 - val_loss: 0.0680 - val_accuracy: 0.7531 - lr: 1.0000e-08\n",
            "Epoch 51/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.7710\n",
            "Epoch 51: val_loss did not improve from 0.06798\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0653 - accuracy: 0.7711 - val_loss: 0.0681 - val_accuracy: 0.7526 - lr: 1.0000e-08\n",
            "Epoch 52/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.7678\n",
            "Epoch 52: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.0658 - accuracy: 0.7678 - val_loss: 0.0681 - val_accuracy: 0.7539 - lr: 1.0000e-09\n",
            "Epoch 53/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.7710\n",
            "Epoch 53: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0651 - accuracy: 0.7710 - val_loss: 0.0680 - val_accuracy: 0.7533 - lr: 1.0000e-09\n",
            "Epoch 54/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.7721\n",
            "Epoch 54: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0651 - accuracy: 0.7721 - val_loss: 0.0680 - val_accuracy: 0.7537 - lr: 1.0000e-09\n",
            "Epoch 55/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.7690\n",
            "Epoch 55: val_loss did not improve from 0.06798\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.0656 - accuracy: 0.7690 - val_loss: 0.0680 - val_accuracy: 0.7539 - lr: 1.0000e-09\n",
            "Epoch 56/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.7701\n",
            "Epoch 56: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0654 - accuracy: 0.7701 - val_loss: 0.0681 - val_accuracy: 0.7531 - lr: 1.0000e-10\n",
            "Epoch 57/90\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.7706\n",
            "Epoch 57: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0652 - accuracy: 0.7704 - val_loss: 0.0681 - val_accuracy: 0.7541 - lr: 1.0000e-10\n",
            "Epoch 58/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.7727\n",
            "Epoch 58: val_loss did not improve from 0.06798\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0650 - accuracy: 0.7727 - val_loss: 0.0681 - val_accuracy: 0.7539 - lr: 1.0000e-10\n",
            "Epoch 59/90\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.7708\n",
            "Epoch 59: val_loss did not improve from 0.06798\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.0654 - accuracy: 0.7708 - val_loss: 0.0681 - val_accuracy: 0.7533 - lr: 1.0000e-10\n"
          ]
        }
      ],
      "source": [
        "# run model for gender\n",
        "age_model.compile(loss='mean_squared_error', optimizer= tensorflow.optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
        "\n",
        "# callbacks\n",
        "log_file_path = base_path + 'age_training.log'\n",
        "csv_logger = CSVLogger(log_file_path, append=False)\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "trained_models_path = base_path + 'age_model_Xception'\n",
        "model_names = trained_models_path + '.{epoch:02d}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
        "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        "\n",
        "age = age_model.fit(train1, verbose=1, callbacks=callbacks, validation_data=test1,  epochs=90,\n",
        "                batch_size=batch_size )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvuXje5k90xW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "56f31744-a290-4d19-f11d-ab9de3933fa3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Train and Validation Loss\n",
        "plt.plot(age.history[\"loss\"],label=\"train loss\")\n",
        "plt.plot(age.history[\"val_loss\"],label=\"val loss\")\n",
        "plt.legend()\n",
        "fig1 = plt.gcf() # I create figure because ı want to save\n",
        "plt.show()\n",
        "plt.draw()\n",
        "#fig1.savefig('ageclfloss.png')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "##Train and Validation Accuracy\n",
        "plt.plot(age.history[\"accuracy\"],label=\"train acc\")\n",
        "plt.plot(age.history[\"val_accuracy\"],label=\"val acc\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "fig2 = plt.gcf()\n",
        "plt.show()\n",
        "plt.draw()\n",
        "#fig2.savefig('ageclfacc.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Aq-85zo900x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86be7595-8e36-4d19-a605-275399f3fcb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples in Training: (18966, 48, 48, 3)\n",
            "Samples in Testing: (4742, 48, 48, 3)\n",
            "y_train shape:  (18966, 2)\n",
            "y_test shape:  (4742, 2)\n",
            "input shape:  (48, 48, 3)\n"
          ]
        }
      ],
      "source": [
        "## Train-Test Split for gender\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_data,gender_classes,test_size=0.2,\n",
        "                                               shuffle=True,random_state=42)\n",
        "\n",
        "#one hot encoding\n",
        "y_train= to_categorical(y_train,num_classes=2)\n",
        "y_test= to_categorical(y_test,num_classes=2) \n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255.\n",
        "\n",
        "print(\"Samples in Training:\",x_train.shape)\n",
        "print(\"Samples in Testing:\",x_test.shape)\n",
        "\n",
        "\n",
        "print(\"y_train shape: \",y_train.shape)\n",
        "print(\"y_test shape: \",y_test.shape)\n",
        "\n",
        "input_shape= x_train.shape[1:] #we don't take the number of samples\n",
        "print(\"input shape: \",input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzmZaT1reRrc"
      },
      "outputs": [],
      "source": [
        "def exit_gender_flow(x) :\n",
        "\n",
        "    previous_block_activation = x\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(312, 5, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(624, 5, padding='same')(x) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    residual = Conv2D(624, 1, strides=2, padding='same')(previous_block_activation)\n",
        "    x = tensorflow.keras.layers.Add()([x, residual])\n",
        "\n",
        "\n",
        "    x = SeparableConv2D(624, 5, padding='same')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = SeparableConv2D(312, 3, padding='same')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(2, activation='sigmoid')(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvjJ6JqB904I"
      },
      "outputs": [],
      "source": [
        "img_input = Input(shape=(48, 48, 3))\n",
        "output = exit_gender_flow(middle_flow(entry_flow(img_input)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJIR9wKZe8N-"
      },
      "outputs": [],
      "source": [
        "gender_model = Model(img_input, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBdXQxNLDVK7"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)     \n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "train2 = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "test2 = test_datagen.flow(x_test, y_test, batch_size=batch_size)\n",
        "gender_model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHUlEOjUMcTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85293d72-fe69-4a58-f84f-3db2d3df7325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "592/593 [============================>.] - ETA: 0s - loss: 0.5072 - accuracy: 0.7480\n",
            "Epoch 1: val_loss improved from inf to 0.58226, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.01.hdf5\n",
            "593/593 [==============================] - 55s 72ms/step - loss: 0.5070 - accuracy: 0.7481 - val_loss: 0.5823 - val_accuracy: 0.6742 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.3781 - accuracy: 0.8304\n",
            "Epoch 2: val_loss improved from 0.58226 to 0.34506, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.02.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.3781 - accuracy: 0.8304 - val_loss: 0.3451 - val_accuracy: 0.8435 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8445\n",
            "Epoch 3: val_loss did not improve from 0.34506\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.3499 - accuracy: 0.8445 - val_loss: 0.3538 - val_accuracy: 0.8402 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.8554\n",
            "Epoch 4: val_loss did not improve from 0.34506\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.3272 - accuracy: 0.8554 - val_loss: 0.3872 - val_accuracy: 0.8113 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8609\n",
            "Epoch 5: val_loss improved from 0.34506 to 0.31479, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.05.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.3167 - accuracy: 0.8609 - val_loss: 0.3148 - val_accuracy: 0.8587 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.8633\n",
            "Epoch 6: val_loss did not improve from 0.31479\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.3081 - accuracy: 0.8633 - val_loss: 0.3203 - val_accuracy: 0.8598 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.8684\n",
            "Epoch 7: val_loss improved from 0.31479 to 0.29018, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.07.hdf5\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 0.2981 - accuracy: 0.8684 - val_loss: 0.2902 - val_accuracy: 0.8699 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.8711\n",
            "Epoch 8: val_loss improved from 0.29018 to 0.28618, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.08.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.2908 - accuracy: 0.8711 - val_loss: 0.2862 - val_accuracy: 0.8720 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.8764\n",
            "Epoch 9: val_loss improved from 0.28618 to 0.28618, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.09.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.2822 - accuracy: 0.8764 - val_loss: 0.2862 - val_accuracy: 0.8737 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8799\n",
            "Epoch 10: val_loss improved from 0.28618 to 0.26617, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.10.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.2769 - accuracy: 0.8799 - val_loss: 0.2662 - val_accuracy: 0.8800 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.8841\n",
            "Epoch 11: val_loss did not improve from 0.26617\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.2693 - accuracy: 0.8841 - val_loss: 0.2744 - val_accuracy: 0.8855 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.8875\n",
            "Epoch 12: val_loss improved from 0.26617 to 0.25894, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.12.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.2635 - accuracy: 0.8875 - val_loss: 0.2589 - val_accuracy: 0.8884 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.8895\n",
            "Epoch 13: val_loss did not improve from 0.25894\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2593 - accuracy: 0.8895 - val_loss: 0.2807 - val_accuracy: 0.8747 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.8873\n",
            "Epoch 14: val_loss improved from 0.25894 to 0.24610, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.14.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.2579 - accuracy: 0.8874 - val_loss: 0.2461 - val_accuracy: 0.8901 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.8913\n",
            "Epoch 15: val_loss did not improve from 0.24610\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2489 - accuracy: 0.8913 - val_loss: 0.2639 - val_accuracy: 0.8798 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.8937\n",
            "Epoch 16: val_loss did not improve from 0.24610\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2487 - accuracy: 0.8937 - val_loss: 0.2488 - val_accuracy: 0.8914 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.8956\n",
            "Epoch 17: val_loss did not improve from 0.24610\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2409 - accuracy: 0.8958 - val_loss: 0.2525 - val_accuracy: 0.8849 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.8968\n",
            "Epoch 18: val_loss did not improve from 0.24610\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2406 - accuracy: 0.8968 - val_loss: 0.2546 - val_accuracy: 0.8910 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9041\n",
            "Epoch 19: val_loss improved from 0.24610 to 0.22810, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.19.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.2220 - accuracy: 0.9041 - val_loss: 0.2281 - val_accuracy: 0.9062 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9113\n",
            "Epoch 20: val_loss improved from 0.22810 to 0.22779, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.20.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.2110 - accuracy: 0.9113 - val_loss: 0.2278 - val_accuracy: 0.9028 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9127\n",
            "Epoch 21: val_loss improved from 0.22779 to 0.22473, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.21.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.2073 - accuracy: 0.9128 - val_loss: 0.2247 - val_accuracy: 0.9057 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9144\n",
            "Epoch 22: val_loss did not improve from 0.22473\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2058 - accuracy: 0.9144 - val_loss: 0.2359 - val_accuracy: 0.8971 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.9155\n",
            "Epoch 23: val_loss improved from 0.22473 to 0.22391, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.23.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.2043 - accuracy: 0.9155 - val_loss: 0.2239 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9162\n",
            "Epoch 24: val_loss did not improve from 0.22391\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.2030 - accuracy: 0.9162 - val_loss: 0.2255 - val_accuracy: 0.9053 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9176\n",
            "Epoch 25: val_loss did not improve from 0.22391\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.2006 - accuracy: 0.9176 - val_loss: 0.2251 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.9181\n",
            "Epoch 26: val_loss improved from 0.22391 to 0.22212, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.26.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.2010 - accuracy: 0.9181 - val_loss: 0.2221 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9166\n",
            "Epoch 27: val_loss improved from 0.22212 to 0.22107, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.27.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.1950 - accuracy: 0.9166 - val_loss: 0.2211 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9193\n",
            "Epoch 28: val_loss did not improve from 0.22107\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1955 - accuracy: 0.9193 - val_loss: 0.2227 - val_accuracy: 0.9030 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9192\n",
            "Epoch 29: val_loss did not improve from 0.22107\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1947 - accuracy: 0.9192 - val_loss: 0.2216 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9212\n",
            "Epoch 30: val_loss improved from 0.22107 to 0.22034, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.30.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.1916 - accuracy: 0.9212 - val_loss: 0.2203 - val_accuracy: 0.9070 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1909 - accuracy: 0.9223\n",
            "Epoch 31: val_loss improved from 0.22034 to 0.21837, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.31.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.1909 - accuracy: 0.9222 - val_loss: 0.2184 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9214\n",
            "Epoch 32: val_loss did not improve from 0.21837\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1911 - accuracy: 0.9214 - val_loss: 0.2201 - val_accuracy: 0.9074 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9201\n",
            "Epoch 33: val_loss did not improve from 0.21837\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1919 - accuracy: 0.9201 - val_loss: 0.2198 - val_accuracy: 0.9081 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9237\n",
            "Epoch 34: val_loss did not improve from 0.21837\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1869 - accuracy: 0.9237 - val_loss: 0.2223 - val_accuracy: 0.9074 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9204\n",
            "Epoch 35: val_loss did not improve from 0.21837\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1931 - accuracy: 0.9204 - val_loss: 0.2202 - val_accuracy: 0.9100 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9235\n",
            "Epoch 36: val_loss did not improve from 0.21837\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1889 - accuracy: 0.9235 - val_loss: 0.2184 - val_accuracy: 0.9104 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9234\n",
            "Epoch 37: val_loss improved from 0.21837 to 0.21805, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.37.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.1871 - accuracy: 0.9235 - val_loss: 0.2181 - val_accuracy: 0.9085 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9226\n",
            "Epoch 38: val_loss did not improve from 0.21805\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1869 - accuracy: 0.9226 - val_loss: 0.2187 - val_accuracy: 0.9091 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9232\n",
            "Epoch 39: val_loss improved from 0.21805 to 0.21776, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.39.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.1838 - accuracy: 0.9232 - val_loss: 0.2178 - val_accuracy: 0.9102 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.9229\n",
            "Epoch 40: val_loss improved from 0.21776 to 0.21757, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.40.hdf5\n",
            "593/593 [==============================] - 42s 71ms/step - loss: 0.1831 - accuracy: 0.9229 - val_loss: 0.2176 - val_accuracy: 0.9100 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9260\n",
            "Epoch 41: val_loss did not improve from 0.21757\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 0.1861 - accuracy: 0.9260 - val_loss: 0.2183 - val_accuracy: 0.9095 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9249\n",
            "Epoch 42: val_loss did not improve from 0.21757\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1862 - accuracy: 0.9249 - val_loss: 0.2176 - val_accuracy: 0.9100 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9249\n",
            "Epoch 43: val_loss did not improve from 0.21757\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1835 - accuracy: 0.9249 - val_loss: 0.2179 - val_accuracy: 0.9085 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9249\n",
            "Epoch 44: val_loss did not improve from 0.21757\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1846 - accuracy: 0.9249 - val_loss: 0.2176 - val_accuracy: 0.9091 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9241\n",
            "Epoch 45: val_loss did not improve from 0.21757\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1832 - accuracy: 0.9241 - val_loss: 0.2178 - val_accuracy: 0.9089 - lr: 1.0000e-06\n",
            "Epoch 46/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9243\n",
            "Epoch 46: val_loss did not improve from 0.21757\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1824 - accuracy: 0.9243 - val_loss: 0.2177 - val_accuracy: 0.9093 - lr: 1.0000e-06\n",
            "Epoch 47/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9259\n",
            "Epoch 47: val_loss did not improve from 0.21757\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1839 - accuracy: 0.9259 - val_loss: 0.2177 - val_accuracy: 0.9087 - lr: 1.0000e-06\n",
            "Epoch 48/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9260\n",
            "Epoch 48: val_loss did not improve from 0.21757\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1826 - accuracy: 0.9260 - val_loss: 0.2178 - val_accuracy: 0.9089 - lr: 1.0000e-06\n",
            "Epoch 49/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1829 - accuracy: 0.9259\n",
            "Epoch 49: val_loss improved from 0.21757 to 0.21754, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.49.hdf5\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 0.1829 - accuracy: 0.9259 - val_loss: 0.2175 - val_accuracy: 0.9100 - lr: 1.0000e-07\n",
            "Epoch 50/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.9261\n",
            "Epoch 50: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 0.1827 - accuracy: 0.9261 - val_loss: 0.2177 - val_accuracy: 0.9093 - lr: 1.0000e-07\n",
            "Epoch 51/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9266\n",
            "Epoch 51: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1821 - accuracy: 0.9266 - val_loss: 0.2178 - val_accuracy: 0.9091 - lr: 1.0000e-07\n",
            "Epoch 52/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9243\n",
            "Epoch 52: val_loss did not improve from 0.21754\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1873 - accuracy: 0.9243 - val_loss: 0.2177 - val_accuracy: 0.9095 - lr: 1.0000e-07\n",
            "Epoch 53/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9250\n",
            "Epoch 53: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1860 - accuracy: 0.9250 - val_loss: 0.2177 - val_accuracy: 0.9091 - lr: 1.0000e-08\n",
            "Epoch 54/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9266\n",
            "Epoch 54: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1858 - accuracy: 0.9266 - val_loss: 0.2176 - val_accuracy: 0.9095 - lr: 1.0000e-08\n",
            "Epoch 55/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9226\n",
            "Epoch 55: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1861 - accuracy: 0.9226 - val_loss: 0.2177 - val_accuracy: 0.9089 - lr: 1.0000e-08\n",
            "Epoch 56/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.9275\n",
            "Epoch 56: val_loss did not improve from 0.21754\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1837 - accuracy: 0.9275 - val_loss: 0.2178 - val_accuracy: 0.9089 - lr: 1.0000e-08\n",
            "Epoch 57/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9231\n",
            "Epoch 57: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1852 - accuracy: 0.9231 - val_loss: 0.2176 - val_accuracy: 0.9091 - lr: 1.0000e-09\n",
            "Epoch 58/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9251\n",
            "Epoch 58: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1838 - accuracy: 0.9251 - val_loss: 0.2176 - val_accuracy: 0.9091 - lr: 1.0000e-09\n",
            "Epoch 59/100\n",
            "592/593 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9205\n",
            "Epoch 59: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1874 - accuracy: 0.9205 - val_loss: 0.2175 - val_accuracy: 0.9093 - lr: 1.0000e-09\n",
            "Epoch 60/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9252\n",
            "Epoch 60: val_loss did not improve from 0.21754\n",
            "\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1838 - accuracy: 0.9252 - val_loss: 0.2178 - val_accuracy: 0.9095 - lr: 1.0000e-09\n",
            "Epoch 61/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9256\n",
            "Epoch 61: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1836 - accuracy: 0.9256 - val_loss: 0.2176 - val_accuracy: 0.9087 - lr: 1.0000e-10\n",
            "Epoch 62/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9232\n",
            "Epoch 62: val_loss did not improve from 0.21754\n",
            "593/593 [==============================] - 40s 67ms/step - loss: 0.1847 - accuracy: 0.9232 - val_loss: 0.2176 - val_accuracy: 0.9091 - lr: 1.0000e-10\n",
            "Epoch 63/100\n",
            "593/593 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9249\n",
            "Epoch 63: val_loss improved from 0.21754 to 0.21744, saving model to /content/drive/My Drive/Emotion_Detection/GENDER_AGE/gender_model_Xception.63.hdf5\n",
            "593/593 [==============================] - 42s 70ms/step - loss: 0.1852 - accuracy: 0.9249 - val_loss: 0.2174 - val_accuracy: 0.9087 - lr: 1.0000e-10\n",
            "Epoch 64/100\n",
            " 98/593 [===>..........................] - ETA: 31s - loss: 0.1822 - accuracy: 0.9260"
          ]
        }
      ],
      "source": [
        "# run model for gender\n",
        "gender_model.compile(loss=\"binary_crossentropy\", optimizer= tensorflow.optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
        "\n",
        "# callbacks\n",
        "log_file_path = base_path + 'gender_training.log'\n",
        "csv_logger = CSVLogger(log_file_path, append=False)\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "trained_models_path = base_path + 'gender_model_Xception'\n",
        "model_names = trained_models_path + '.{epoch:02d}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
        "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        "\n",
        "gender = gender_model.fit(train2, verbose=1, callbacks=callbacks, validation_data=test2,  epochs=num_epochs,\n",
        "                batch_size=batch_size )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k37M3OGrMcXX"
      },
      "outputs": [],
      "source": [
        "#Train and Validation Loss\n",
        "plt.plot(gender.history[\"loss\"],label=\"train loss\")\n",
        "plt.plot(gender.history[\"val_loss\"],label=\"val loss\")\n",
        "plt.legend()\n",
        "fig1 = plt.gcf() # I create figure because ı want to save\n",
        "plt.show()\n",
        "plt.draw()\n",
        "#fig1.savefig('genderclfloss.png')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "##Train and Validation Accuracy\n",
        "plt.plot(gender.history[\"accuracy\"],label=\"train acc\")\n",
        "plt.plot(gender.history[\"val_accuracy\"],label=\"val acc\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "fig2 = plt.gcf()\n",
        "plt.show()\n",
        "plt.draw()\n",
        "#fig2.savefig('genderclfacc.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dMA2P_bMVQ1"
      },
      "outputs": [],
      "source": [
        "## gender results\n",
        "results= gender_model.evaluate(x_test,y_test)\n",
        "print(\"Results: \",results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNDv4mNfMslY"
      },
      "outputs": [],
      "source": [
        "#age= to_categorical(age,num_classes=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjahBuCQJfC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "gender_age_training_xception.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}